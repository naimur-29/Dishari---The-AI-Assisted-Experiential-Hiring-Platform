{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyngrok\nfrom pyngrok import ngrok\n\nimport os\nimport sys\nimport subprocess\nimport contextlib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T04:57:21.912955Z","iopub.execute_input":"2025-05-07T04:57:21.913172Z","iopub.status.idle":"2025-05-07T04:57:32.117048Z","shell.execute_reply.started":"2025-05-07T04:57:21.913150Z","shell.execute_reply":"2025-05-07T04:57:32.116254Z"}},"outputs":[{"name":"stdout","text":"Collecting pyngrok\n  Downloading pyngrok-7.2.7-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.10/site-packages (from pyngrok) (6.0.1)\nDownloading pyngrok-7.2.7-py3-none-any.whl (23 kB)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.2.7\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"os.chdir('/kaggle/working/')\n\nif not os.path.exists('/kaggle/tmp'):\n   os.mkdir('/kaggle/tmp')\nos.chdir('/kaggle/tmp/')\n\nprint(os.getcwd())\n\ndef run(commands, is_out=True):\n    for command in commands:\n        if is_out:\n            with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\n                for line in sp.stdout:\n                    line = line.decode(\"utf-8\", errors = \"replace\")\n                    if \"undefined reference\" in line:\n                        raise RuntimeError(\"Failed Processing.\")\n                    print(line, flush = True, end = \"\")\n            pass\n        else:\n            with subprocess.Popen(command, shell = True, stdout = subprocess.DEVNULL, stderr = subprocess.DEVNULL) as sp:\n                sp.wait()\n    pass\npass","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T04:58:25.086054Z","iopub.execute_input":"2025-05-07T04:58:25.086623Z","iopub.status.idle":"2025-05-07T04:58:25.093981Z","shell.execute_reply.started":"2025-05-07T04:58:25.086586Z","shell.execute_reply":"2025-05-07T04:58:25.093095Z"}},"outputs":[{"name":"stdout","text":"/kaggle/tmp\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"commands = [\n        \"curl -fsSL https://ollama.com/install.sh | sh\",\n]\nrun(commands)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T05:02:52.597401Z","iopub.execute_input":"2025-05-07T05:02:52.598041Z","iopub.status.idle":"2025-05-07T05:03:36.609321Z","shell.execute_reply.started":"2025-05-07T05:02:52.598012Z","shell.execute_reply":"2025-05-07T05:03:36.607829Z"}},"outputs":[{"name":"stdout","text":">>> Installing ollama to /usr/local\n>>> Downloading Linux amd64 bundle\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/subprocess.py:961: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n  self.stdout = io.open(c2pread, 'rb', bufsize)\n","output_type":"stream"},{"name":"stdout","text":"######################################################################## 100.0%\n>>> Creating ollama user...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n>>> NVIDIA GPU installed.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"os.system(\"OLLAMA_HOST=0.0.0.0 /usr/local/bin/ollama serve &\")\nos.system(\"ollama list\")","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-05-07T05:03:44.775015Z","iopub.execute_input":"2025-05-07T05:03:44.775384Z","iopub.status.idle":"2025-05-07T05:03:44.811613Z","shell.execute_reply.started":"2025-05-07T05:03:44.775356Z","shell.execute_reply":"2025-05-07T05:03:44.810538Z"}},"outputs":[{"name":"stdout","text":"Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\nYour new public key is: \n\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIEg1gMputR39raDvs41U7tlX1m9x30d+D8mjVVuhqjna\n\n","output_type":"stream"},{"name":"stderr","text":"Error: could not connect to ollama app, is it running?\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"256"},"metadata":{}},{"name":"stderr","text":"2025/05/07 05:03:44 routes.go:1233: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2025-05-07T05:03:44.808Z level=INFO source=images.go:463 msg=\"total blobs: 0\"\ntime=2025-05-07T05:03:44.808Z level=INFO source=images.go:470 msg=\"total unused blobs removed: 0\"\ntime=2025-05-07T05:03:44.808Z level=INFO source=routes.go:1300 msg=\"Listening on [::]:11434 (version 0.6.8)\"\ntime=2025-05-07T05:03:44.808Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-05-07T05:03:45.140Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-fcc80cf8-9876-c647-0a35-b2750b072ec8 library=cuda variant=v12 compute=7.5 driver=12.6 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\ntime=2025-05-07T05:03:45.140Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-833469d4-6cee-7a16-4739-a220a1101bcd library=cuda variant=v12 compute=7.5 driver=12.6 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Set the authentication token (optional, but recommended)\nngrok.set_auth_token(\"2frwIgdDtIm9aijbShsfTqtVzrV_7TJDbr4tHvUSXuYEBe9mB\")\n\n# Suppress stdout and stderr\nwith open(os.devnull, \"w\") as devnull, contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):\n    public_url = ngrok.connect(addr=\"127.0.0.1:11434\", proto=\"http\")\n\n# If you still need the public URL without printing extra logs:\nprint(\"Public URL:\", str(public_url))","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-05-07T05:03:55.292476Z","iopub.execute_input":"2025-05-07T05:03:55.293022Z","iopub.status.idle":"2025-05-07T05:03:57.171674Z","shell.execute_reply.started":"2025-05-07T05:03:55.292994Z","shell.execute_reply":"2025-05-07T05:03:57.170819Z"}},"outputs":[{"name":"stdout","text":"Public URL: NgrokTunnel: \"https://278e-34-127-66-141.ngrok-free.app\" -> \"http://127.0.0.1:11434\"    \n[GIN] 2025/05/07 - 05:04:03 | 200 |     101.624µs |  103.157.134.42 | GET      \"/\"\n[GIN] 2025/05/07 - 05:04:03 | 404 |       7.479µs |  103.157.134.42 | GET      \"/favicon.ico\"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"init_commands = [\n    \"ollama pull gemma3:4b\"\n]\nrun(init_commands, False)\n\ncommands = [\n        # \"ollama pull deepseek-r1:7b\",\n        \"ollama list\",\n        # \"ollama ps\",\n        # \"ollama rm qwen2.5-coder:7b\",\n        # \"telnet 0.0.0.0 11434\",\n        # \"ollama pull phi4\",\n        # \"ollama pull wizard-vicuna-uncensored:13b\",\n        #\"ollama pull phi3\",\n        #\"ollama pull mistral\",\n        # \"ollama run deepseek-r1:7b \\\"create 10 sentences that ends with apple\\\"\",\n        # \"ollama run deepseek-r1:14b \\\"create 10 sentences that ends with apple\\\"\",\n        #\"curl http://127.0.0.1:11434/api/chat -d '{\\\"model\\\": \\\"llama3\\\", \\\"stream\\\": false, \\\"messages\\\": [{ \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"create 10 sentences that ends with apple\\\" }]}'\"\n]\nrun(commands)","metadata":{"trusted":true,"scrolled":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2025-05-07T06:24:13.412791Z","iopub.execute_input":"2025-05-07T06:24:13.413574Z","iopub.status.idle":"2025-05-07T06:24:33.865923Z","shell.execute_reply.started":"2025-05-07T06:24:13.413518Z","shell.execute_reply":"2025-05-07T06:24:33.865058Z"}},"outputs":[{"name":"stdout","text":"[GIN] 2025/05/07 - 06:24:13 | 200 |      76.943µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/05/07 - 06:24:13 | 200 |  422.204829ms |       127.0.0.1 | POST     \"/api/pull\"\n[GIN] 2025/05/07 - 06:24:13 | 200 |      43.451µs |       127.0.0.1 | HEAD     \"/\"\n","output_type":"stream"},{"name":"stderr","text":"time=2025-05-07T06:24:14.506Z level=INFO source=download.go:177 msg=\"downloading aeda25e63ebd in 16 208 MB part(s)\"\ntime=2025-05-07T06:24:21.769Z level=INFO source=download.go:177 msg=\"downloading b6ae5839783f in 1 489 B part(s)\"\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2025/05/07 - 06:24:33 | 200 | 19.950146723s |       127.0.0.1 | POST     \"/api/pull\"\n[GIN] 2025/05/07 - 06:24:33 | 200 |      35.357µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/05/07 - 06:24:33 | 200 |     697.301µs |       127.0.0.1 | GET      \"/api/tags\"\nNAME             ID              SIZE      MODIFIED               \ngemma3:4b        a2af6cc3eb7f    3.3 GB    Less than a second ago    \nqwen3:30b-a3b    2ee832bc15b5    18 GB     20 seconds ago            \ngemma3:12b       f4031aab637d    8.1 GB    About an hour ago         \n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Kill existing processes\nos.system(\"pkill -f 'ollama'\")\n# os.system(\"pkill -f 'ngrok'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"hello world\")\nrun([\"ollama list\"])\nrun([\"ollama ps\"])\n\n\n# run([\"ollama stop qwen3:30b-a3b\"])","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-05-07T11:02:32.652389Z","iopub.execute_input":"2025-05-07T11:02:32.652989Z","iopub.status.idle":"2025-05-07T11:02:32.707179Z","shell.execute_reply.started":"2025-05-07T11:02:32.652958Z","shell.execute_reply":"2025-05-07T11:02:32.706556Z"}},"outputs":[{"name":"stdout","text":"hello world\n[GIN] 2025/05/07 - 11:02:32 | 200 |      42.509µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/05/07 - 11:02:32 | 200 |     668.047µs |       127.0.0.1 | GET      \"/api/tags\"\nNAME             ID              SIZE      MODIFIED    \ngemma3:4b        a2af6cc3eb7f    3.3 GB    5 hours ago    \nqwen3:30b-a3b    2ee832bc15b5    18 GB     5 hours ago    \ngemma3:12b       f4031aab637d    8.1 GB    6 hours ago    \n[GIN] 2025/05/07 - 11:02:32 | 200 |      28.697µs |       127.0.0.1 | HEAD     \"/\"\n[GIN] 2025/05/07 - 11:02:32 | 200 |       13.29µs |       127.0.0.1 | GET      \"/api/ps\"\nNAME    ID    SIZE    PROCESSOR    UNTIL \n","output_type":"stream"}],"execution_count":31}]}